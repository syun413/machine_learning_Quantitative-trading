{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "機器學習-分析以太幣數據",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 量化交易 X 機器學習"
      ],
      "metadata": {
        "id": "WhLyZMBOxfWP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##（一）下載資料集\n",
        "將會使用到的資料（train.csv、test.csv）下載下來。"
      ],
      "metadata": {
        "id": "VzArUJaf1D90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 18WzLy-pv8niLX5SAST_ImM-2zISSOXsV"
      ],
      "metadata": {
        "id": "nbU7G1vQ1cDo",
        "outputId": "c4ae7f2c-129f-4429-ec39-967004b575db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=18WzLy-pv8niLX5SAST_ImM-2zISSOXsV\n",
            "To: /content/train.csv\n",
            "100% 200M/200M [00:02<00:00, 96.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## （二）導入套件\n",
        "引入使用到的套件\n"
      ],
      "metadata": {
        "id": "sDePGjS40-sI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas_ta"
      ],
      "metadata": {
        "id": "5bxELxOk0j4-",
        "outputId": "e5a9b4b8-f5df-4b47-e265-872887066b0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas_ta\n",
            "  Downloading pandas_ta-0.3.14b.tar.gz (115 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▉                             | 10 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 20 kB 30.0 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 30 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 40 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 51 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 61 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 71 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 81 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 92 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 102 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 112 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 115 kB 7.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from pandas_ta) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pandas_ta) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas->pandas_ta) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->pandas_ta) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->pandas_ta) (1.15.0)\n",
            "Building wheels for collected packages: pandas-ta\n",
            "  Building wheel for pandas-ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pandas-ta: filename=pandas_ta-0.3.14b0-py3-none-any.whl size=218923 sha256=7d6e9d6e768ecb8a05978176fc849bbfb9d75df36b87966e297fb76a67a72a20\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/81/f0/cca85757840e4616a2c6b9fe12569d97d324c27cac60724c58\n",
            "Successfully built pandas-ta\n",
            "Installing collected packages: pandas-ta\n",
            "Successfully installed pandas-ta-0.3.14b0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JThdnrkqxWoc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import pandas_ta as ta\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.linear_model import LinearRegression, Ridge, LogisticRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import SimpleRNN, LSTM, GRU, Dense, Dropout, LeakyReLU\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "from xgboost import XGBClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##（三）設定參數\n",
        "將參數整理在一起，方便管理\n",
        "\n"
      ],
      "metadata": {
        "id": "Xc2or2i10zPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_file_path = 'train.csv'\n",
        "train_ratio = 0.8 #可以改成任意0到到1之間之間的數值（不包括0跟跟1）\n",
        "minutes_combined = 60*24 #可以改成任意正整數\n",
        "rows_used = 10 #可以改成任意正整數\n",
        "method = 'Classification' #有'Regression', 'RNN', 'Classification' 可以做選用\n",
        "regression_method = 'RidgeRegression' #有'LinearRegression', 'RidgeRegression' 可以做選用\n",
        "ridge_regression_alpha = 0.1 #可以改成任意大於0的數值\n",
        "rnn_method = 'SimpleRNN' #有'SimpleRNN', 'LSTM', 'GRU' 可以做選用\n",
        "classification_method = 'XGBClassifier' #有'LogisticRegression', 'RandomForest', 'XGBClassifier' 可以做選用\n",
        "normalize_method = 'normalize_by_change' #有'z_normalization', 'min_max_normalization', 'normalize_by_change' 可以做選用\n",
        "use_ta = False"
      ],
      "metadata": {
        "id": "zVvTBvFB7GW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##（四）讀取及區分資料\n",
        "讀取之前下載好的資料以便之後做使用，並將其區分為訓練及測試資料。"
      ],
      "metadata": {
        "id": "1xN1wi_U7qWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(train_file_path)\n",
        "print(len(df))\n",
        "print(df.head())\n",
        "\n",
        "# Todo: drop timestamp column\n",
        "df = df.drop(columns=['timestamp'])\n",
        "print(len(df))\n",
        "print(df.head())\n",
        "\n",
        "df = df[:int(train_ratio * len(df))]\n",
        "\n",
        "original_train_df = df[:int(train_ratio * len(df))]\n",
        "original_valid_df = df[int(train_ratio * len(df)):]"
      ],
      "metadata": {
        "id": "qMvSE6cgBCZz",
        "outputId": "d30108a8-a628-4176-d4d2-fa38d23aaea9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1956282\n",
            "    timestamp  Count       Open  ...      Volume          VWAP      Close\n",
            "0  1514764860  229.0  13835.194  ...   31.550062  13827.062093  13850.176\n",
            "1  1514764920  235.0  13835.036  ...   31.046432  13840.362591  13828.102\n",
            "2  1514764980  528.0  13823.900  ...   55.061820  13806.068014  13801.314\n",
            "3  1514765040  435.0  13802.512  ...   38.780529  13783.598101  13768.040\n",
            "4  1514765100  742.0  13766.000  ...  108.501637  13735.586842  13724.914\n",
            "\n",
            "[5 rows x 8 columns]\n",
            "1956282\n",
            "   Count       Open     High       Low      Volume          VWAP      Close\n",
            "0  229.0  13835.194  14013.8  13666.11   31.550062  13827.062093  13850.176\n",
            "1  235.0  13835.036  14052.3  13680.00   31.046432  13840.362591  13828.102\n",
            "2  528.0  13823.900  14000.4  13601.00   55.061820  13806.068014  13801.314\n",
            "3  435.0  13802.512  13999.0  13576.28   38.780529  13783.598101  13768.040\n",
            "4  742.0  13766.000  13955.9  13554.44  108.501637  13735.586842  13724.914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##（五）處理訓練及測試資料\n",
        "將訓練及測試資料做處理"
      ],
      "metadata": {
        "id": "UeU88hdWBllP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = original_train_df\n",
        "valid_df = original_valid_df\n",
        "\n",
        "pd.options.mode.chained_assignment = None\n",
        "\n",
        "if (minutes_combined > 1):\n",
        "  train_minute_list = [i for i in range(1, len(train_df)) if i % minutes_combined == 0]\n",
        "  valid_minute_list = [i for i in range(len(train_df), len(df)) if i % minutes_combined == 0]\n",
        "\n",
        "  for index, minute in enumerate(train_minute_list):\n",
        "    temp_df = train_df.loc[(minute-minutes_combined):minute] \n",
        "    train_df.loc[index,:] = np.array([temp_df['Count'].sum(), temp_df['Open'].iloc[0], temp_df['High'].max(), temp_df['Low'].min(), \\\n",
        "      temp_df['Volume'].sum(), temp_df['VWAP'].mul(temp_df['Volume']).sum()/temp_df['Volume'].sum(), temp_df['Close'].iloc[-1]])\n",
        "  train_df = train_df.loc[:len(train_minute_list)]\n",
        "  print(\"train data processing finished\")\n",
        "\n",
        "  valid_df = pd.concat([original_train_df.iloc[-10:], valid_df], axis=0)\n",
        "\n",
        "  for index, minute in enumerate(valid_minute_list):\n",
        "    temp_df = valid_df.loc[(minute-minutes_combined):minute] \n",
        "    valid_df.loc[len(original_train_df)+index,:] = np.array([temp_df['Count'].sum(), temp_df['Open'].iloc[0], temp_df['High'].max(), temp_df['Low'].min(), \\\n",
        "      temp_df['Volume'].sum(), temp_df['VWAP'].mul(temp_df['Volume']).sum()/temp_df['Volume'].sum(), temp_df['Close'].iloc[-1]])\n",
        "  valid_df = valid_df.loc[len(original_train_df):len(original_train_df)+len(valid_minute_list)]\n",
        "  print(\"valid data processing finished\")\n",
        "\n",
        "if (use_ta):\n",
        "  valid_df = pd.concat([train_df.iloc[-30:], valid_df], axis=0)\n",
        "\n",
        "  train_df['MA10'] = train_df.ta.sma(10)\n",
        "  train_df['MA30'] = train_df.ta.sma(30)\n",
        "  train_df.ta.stoch(high='High', low='Low', k=14, d=3, append=True)\n",
        "  train_df = train_df[30:]\n",
        "\n",
        "  # Todo: do the same for valid data\n",
        "  valid_df['MA10'] = valid_df.ta.sma(10)\n",
        "  valid_df['MA30'] = valid_df.ta.sma(30)\n",
        "  valid_df.ta.stoch(high='High', low='Low', k=14, d=3, append=True)\n",
        "  valid_df = valid_df[30:]"
      ],
      "metadata": {
        "id": "WICs705aDEJi",
        "outputId": "432c19cb-1636-4482-eb4d-b205a5508870",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train data processing finished\n",
            "valid data processing finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##（六）資料標準化\n",
        "資料往往需要經過標準化才能提升模型的表現"
      ],
      "metadata": {
        "id": "fIphKzyCZLu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_Y_train = np.array(train_df['Close'])[rows_used:]\n",
        "original_Y_valid = np.array(valid_df['Close'])[rows_used:]\n",
        "\n",
        "normalized_train_df = train_df\n",
        "normalized_valid_df = valid_df\n",
        "normalized_train_close = train_df['Close']\n",
        "normalized_valid_close = valid_df['Close']\n",
        "\n",
        "if (normalize_method):\n",
        "\tif (normalize_method == 'z_normalization'):\n",
        "\t\ttrain_mean = train_df.mean()\n",
        "\t\ttrain_std = train_df.std()\n",
        "\t\tnormalized_train_df = (normalized_train_df - train_mean)/train_std\n",
        "\t\tnormalized_valid_df = (normalized_valid_df - train_mean)/train_std\n",
        "\t\tnormalized_train_close = normalized_train_df['Close']\n",
        "\t\tnormalized_valid_close = normalized_valid_df['Close']\n",
        "\tif (normalize_method == 'min_max_normalization'):\n",
        "\t\ttrain_min = train_df.min()\n",
        "\t\ttrain_max = train_df.max()\n",
        "\t\tnormalized_train_df = (normalized_train_df - train_min)/(train_max - train_min)\n",
        "\t\tnormalized_valid_df = (normalized_valid_df - train_min)/(train_max - train_min)\n",
        "\t\tnormalized_train_close = normalized_train_df['Close']\n",
        "\t\tnormalized_valid_close = normalized_valid_df['Close']\n",
        "\tif (normalize_method == 'normalize_by_change'):\n",
        "\t\tnormalized_train_df = normalized_train_df.div(normalized_train_df.shift(1))\n",
        "\t\tnormalized_train_close = normalized_train_close.div(normalized_train_close.shift(1))\n",
        "\t\tnormalized_train_df.iloc[0] = 1 \n",
        "\t\tnormalized_train_close.iloc[0]= 1\n",
        "\t\tnormalized_valid_df = normalized_valid_df.div(normalized_valid_df.shift(1))\n",
        "\t\tnormalized_valid_close = normalized_valid_close.div(normalized_valid_close.shift(1))\n",
        "\t\tnormalized_valid_df.iloc[0] = valid_df.iloc[0].div(train_df.iloc[-1])\n",
        "\t\tnormalized_valid_close.iloc[0] = valid_df['Close'].iloc[0]/train_df['Close'].iloc[-1]\n",
        "\t\t#print(normalized_train_close[:5])\n",
        "\t\t#print(normalized_valid_close[:5])\n",
        "\n",
        "X_train = np.array(normalized_train_df[:-rows_used])\n",
        "for i in range(1, rows_used):\n",
        "\tX_train = np.append(X_train, np.array(normalized_train_df[i:-rows_used+i]), axis=1)\n",
        "\n",
        "Y_train = np.array(normalized_train_close)[rows_used:]\n",
        "#print(X_train.shape)\n",
        "#print(X_train[:5])\n",
        "#print(Y_train.shape)\n",
        "#print(Y_train[:5])\n",
        "\n",
        "X_valid = np.array(normalized_valid_df[:-rows_used])\n",
        "for i in range(1, rows_used):\n",
        "\tX_valid = np.append(X_valid, np.array(normalized_valid_df[i:-rows_used+i]), axis=1)\n",
        "\n",
        "Y_valid = np.array(normalized_valid_close)[rows_used:]\n",
        "#print(X_valid.shape)\n",
        "#print(X_valid[:5])\n",
        "#print(Y_valid.shape)\n",
        "#print(Y_valid[:5])"
      ],
      "metadata": {
        "id": "jT1FEQOPZLRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##（七）訓練模型\n"
      ],
      "metadata": {
        "id": "0rxyA7FqZv0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_pred = valid_pred = []\n",
        "\n",
        "if (method == 'Regression'):\n",
        "\tif (regression_method == 'LinearRegression'):\n",
        "\t\tmodel = LinearRegression()\n",
        "\telif (regression_method == 'RidgeRegression'):\n",
        "\t\tmodel = Ridge(alpha=ridge_regression_alpha)\n",
        "\t# Todo: fit the model\n",
        "\tmodel.fit(X_train, Y_train)\n",
        "\n",
        "\tprint(model.coef_)\n",
        "\tprint(model.intercept_)\n",
        "\n",
        "\ttrain_pred = model.predict(X_train)\n",
        "\tvalid_pred = model.predict(X_valid)\n",
        "elif (method == 'RNN'):\n",
        "\tX_train = np.reshape(X_train, (X_train.shape[0], rows_used, int(X_train.shape[1]/rows_used)))\n",
        "\tX_valid = np.reshape(X_valid, (X_valid.shape[0], rows_used, int(X_valid.shape[1]/rows_used)))\n",
        "\tmodel = Sequential()\n",
        "\tif (rnn_method == 'SimpleRNN'):\n",
        "\t\tmodel.add(SimpleRNN(units=30, input_shape=(X_train.shape[1],X_train.shape[2])))\n",
        "\t\tmodel.add(Dropout(0.3))\n",
        "\telif (rnn_method == 'LSTM'):\n",
        "\t\tmodel.add(LSTM(units=30, input_shape=(X_train.shape[1],X_train.shape[2])))\n",
        "\t\tmodel.add(Dropout(0.3))\n",
        "\telif (rnn_method == 'GRU'):\n",
        "\t\tmodel.add(GRU(units=30, input_shape=(X_train.shape[1],X_train.shape[2])))\n",
        "\t\tmodel.add(Dropout(0.3))\n",
        "\tmodel.add(Dense(1, activation='elu'))\n",
        "\tmodel.compile(loss=\"mse\", optimizer=\"adam\")\n",
        "\tcallback = EarlyStopping(monitor=\"val_loss\", patience=5, verbose=1, mode=\"auto\")\n",
        "\thistory = model.fit(X_train, Y_train, epochs=100, batch_size=128, validation_data=(X_valid, Y_valid), callbacks=[callback])\n",
        "\ttrain_pred = model.predict(X_train).flatten()\n",
        "\tvalid_pred = model.predict(X_valid).flatten()\n",
        "elif (method == 'Classification'):\n",
        "\tif (normalize_method == 'z_normalization' or normalize_method == 'min_max_normalization'):\n",
        "\t\tY_train = pd.DataFrame(Y_train)\n",
        "\t\tY_train = Y_train.div(Y_train.shift(1))\n",
        "\t\tY_train.iloc[0] = 1\n",
        "\t\tY_train = np.array(Y_train)\n",
        "\t\tY_valid = pd.DataFrame(Y_valid)\n",
        "\t\tY_valid = Y_valid.div(Y_valid.shift(1))\n",
        "\t\tY_valid.iloc[0] = 1\n",
        "\t\tY_valid = np.array(Y_valid)\n",
        "\tincrease_index_list = [index for index,value in enumerate(Y_train) if value > 1]\n",
        "\tdecrease_index_list = [index for index,value in enumerate(Y_train) if value <= 1]\n",
        "\tY_train[increase_index_list] = int(1)\n",
        "\tY_train[decrease_index_list] = int(0)\n",
        "\tincrease_index_list = [index for index,value in enumerate(Y_valid) if value > 1]\n",
        "\tdecrease_index_list = [index for index,value in enumerate(Y_valid) if value <= 1]\n",
        "\tY_valid[increase_index_list] = int(1)\n",
        "\tY_valid[decrease_index_list] = int(0)\n",
        "\t\n",
        "\tif (classification_method == 'LogisticRegression'):\n",
        "\t\tmodel = LogisticRegression(verbose=1, n_jobs=-1)\n",
        "\telif (classification_method == 'RandomForest'):\n",
        "\t\tmodel = RandomForestClassifier(n_estimators=100, max_depth=10, verbose=1, n_jobs=-1)\n",
        "\telif (classification_method == 'XGBClassifier'):\n",
        "\t\tmodel = XGBClassifier(n_estimators=11, eta='0.01', max_depth=10, subsample=0.8, colsample_bytree=1, gamma=3, \\\n",
        "\t\t\teval_metric='auc', tree_method='hist', n_jobs=-1, verbosity=1, use_label_encoder =False)\n",
        "\n",
        "\t# Todo: fit and predict the model\n",
        "\tmodel.fit(X_train, Y_train)\n",
        "\ttrain_pred = model.predict(X_train)\n",
        "\tvalid_pred = model.predict(X_valid)"
      ],
      "metadata": {
        "id": "XN9cQg1JZvKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##（八）衡量模型結果"
      ],
      "metadata": {
        "id": "qCF_vZnOaGZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if (method == 'Regression' or method == 'RNN'):\n",
        "  if (normalize_method == 'normalize_by_change'):\n",
        "    #print(valid_pred[:10])\n",
        "    #print(np.array(valid_df['Close'])[rows_used-1:-1][:10])\n",
        "    #print(len(valid_pred))\n",
        "    #print(len(np.array(valid_df['Close'])[rows_used-1:-1]))\n",
        "    train_pred = train_pred * np.array(train_df['Close'])[rows_used-1:-1]\n",
        "    valid_pred = valid_pred * np.array(valid_df['Close'])[rows_used-1:-1]\n",
        "    #print(valid_pred[:10])\n",
        "  elif (normalize_method == 'z_normalization'):\n",
        "    train_pred = train_pred * train_df['Close'].std() + train_df['Close'].mean()\n",
        "    valid_pred = valid_pred * train_df['Close'].std() + train_df['Close'].mean()\n",
        "  elif (normalize_method == 'min_max_normalization'):\n",
        "    train_pred = train_pred * (train_df['Close'].max()-train_df['Close'].min()) + train_df['Close'].min()\n",
        "    valid_pred = valid_pred * (train_df['Close'].max()-train_df['Close'].min()) + train_df['Close'].min()\n",
        "  print(\"Train RMSE: {}\".format(mean_squared_error(original_Y_train, train_pred, squared=False)))\n",
        "  print(\"Train MAE: {}\".format(mean_absolute_error(original_Y_train, train_pred)))\n",
        "  print(\"Validation RMSE: {}\".format(mean_squared_error(original_Y_valid, valid_pred, squared=False)))\n",
        "  print(\"Validation MAE: {}\".format(mean_absolute_error(original_Y_valid, valid_pred)))\n",
        "  plt.plot([i for i in range(1,len(original_Y_valid)-1)], original_Y_valid[1:-1])\n",
        "  plt.plot([i for i in range(1,len(original_Y_valid)-1)], valid_pred[1:-1])\n",
        "  plt.title('Predict Curve')\n",
        "  plt.legend(['real', 'predict'], loc='upper left')\n",
        "  plt.show()\n",
        "  plt.plot([i for i in range(1,50)], original_Y_valid[1:50])\n",
        "  plt.plot([i for i in range(1,50)], valid_pred[1:50])\n",
        "  plt.title('Predict Curve')\n",
        "  plt.legend(['real', 'predict'], loc='upper left')\n",
        "  plt.show()\n",
        "elif (method == 'Classification'):\n",
        "  target_names = ['Decrease', 'Increase']\n",
        "  #print(train_pred)\n",
        "  print(classification_report(Y_train, train_pred, target_names=target_names, labels=[0,1]))\n",
        "  print(classification_report(Y_valid, valid_pred, target_names=target_names, labels=[0,1]))"
      ],
      "metadata": {
        "id": "4jRINR10aLzd",
        "outputId": "94c33df5-e473-4565-8caa-22ab1a5c1c59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Decrease       0.97      0.99      0.98       419\n",
            "    Increase       0.99      0.98      0.98       441\n",
            "\n",
            "    accuracy                           0.98       860\n",
            "   macro avg       0.98      0.98      0.98       860\n",
            "weighted avg       0.98      0.98      0.98       860\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Decrease       0.48      0.47      0.47        88\n",
            "    Increase       0.61      0.62      0.62       120\n",
            "\n",
            "    accuracy                           0.56       208\n",
            "   macro avg       0.55      0.55      0.55       208\n",
            "weighted avg       0.56      0.56      0.56       208\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
